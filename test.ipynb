{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install facenet-pytorch #../input/package4/facenet_pytorch-2.0.1-py3-none-any.whl\n",
    "\n",
    "import sys, os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "from torchvision.transforms import Normalize, RandomHorizontalFlip, ToTensor, ToPILImage, Compose, Resize\n",
    "from sklearn.metrics import log_loss\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
    "    h, w = img.shape[:2]\n",
    "    if w > h:\n",
    "        h = h * size // w\n",
    "        w = size\n",
    "    else:\n",
    "        w = w * size // h\n",
    "        h = size\n",
    "\n",
    "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
    "    make_square_image(resized)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def make_square_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    top = 0\n",
    "    bottom = size - h\n",
    "    left = 0\n",
    "    right = size - w\n",
    "    return cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "class Video_reader:\n",
    "    def extract_video(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            if ret==True:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        cap.release()\n",
    "        assert len(frames) != 0\n",
    "        return np.array(frames)\n",
    "        \n",
    "        \n",
    "    def extract_one_frame(self, video_path, frame_index):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)  #设置要获取的帧号\n",
    "        _, frame=cap.read()\n",
    "        cap.release()\n",
    "        if _:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            return frame\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class Cache_loader:    \n",
    "    def extract_video(self, video_path):\n",
    "        filename = video_path.split('/')[-1].split('.')[0]\n",
    "        cache_path = '/data1/data/deepfake/faces/'+filename\n",
    "        if os.path.exists(cache_path):\n",
    "            ret = {}\n",
    "            for root,subdirs,files in os.walk(cache_path):\n",
    "                for file in files:\n",
    "                    face = cv2.cvtColor(np.load(os.path.join(root, file)), cv2.COLOR_BGR2RGB)\n",
    "                    ret[int(file.split(\".\")[0])] = isotropically_resize_image(face, 224)\n",
    "            return ret\n",
    "        else:\n",
    "            raise \"cache not found\"\n",
    "            \n",
    "    def get_faces(self, cache_path):\n",
    "        faces = [cv2.cvtColor(np.load(fn), cv2.COLOR_BGR2RGB) for fn in cache_path]\n",
    "        return [isotropically_resize_image(face, 224) for face in faces]\n",
    "        \n",
    "        \n",
    "\n",
    "class Face_extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _get_boundingbox(self, bbox, width, height, scale=1.2, minsize=None):\n",
    "        x1, y1, x2, y2 = bbox[:4]\n",
    "        if not 0.33 < (x2-x1)/(y2-y1) < 3:\n",
    "            return np.array([0,0,0,0])\n",
    "        size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "        if minsize:\n",
    "            if size_bb < minsize:\n",
    "                size_bb = minsize\n",
    "        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        x1 = max(int(center_x - size_bb / 2), 0)\n",
    "        y1 = max(int(center_y - size_bb / 2), 0)\n",
    "        size_bb = min(width - x1, size_bb)\n",
    "        size_bb = min(height - y1, size_bb)\n",
    "\n",
    "        return np.array([x1,y1,x1+size_bb,y1+size_bb]).astype(int)\n",
    "    \n",
    "    \n",
    "    def _rectang_crop(self, image, bbox):\n",
    "        height, width = image.shape[:2]\n",
    "        l,t,r,b = self._get_boundingbox(bbox, width, height) \n",
    "        return image[t:b, l:r]\n",
    "    \n",
    "    def _get(images):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_faces(self, images, with_person_num = False, only_one = True):\n",
    "        faces, nums = self._get(images)\n",
    "        if only_one:\n",
    "            faces = [face[0] for face in faces if len(face)>0]\n",
    "            nums = [num for num, face in zip(nums,faces) if len(face)>0]\n",
    "        if with_person_num:\n",
    "            faces = (faces, nums)\n",
    "        return faces\n",
    "    \n",
    "    \n",
    "    def get_face(self, image, with_person_num = False, only_one = True):\n",
    "        faces, nums = self.get_faces(np.array([image]), with_person_num=True, only_one=False)\n",
    "        faces, nums = faces[0], nums[0]\n",
    "        if only_one:\n",
    "            if len(faces)>0:\n",
    "                faces = faces[0]\n",
    "            else:\n",
    "                faces = None\n",
    "        if with_person_num:\n",
    "            faces = (faces, nums)\n",
    "        return faces\n",
    "    \n",
    "    \n",
    "class MTCNN_extractor(Face_extractor):\n",
    "    def __init__(self, device = 'cuda:0' if torch.cuda.is_available() else 'cpu', down_sample = 2):\n",
    "        self.extractor = MTCNN(keep_all=True, device=device, min_face_size=80//down_sample).eval()\n",
    "        self.down_sample = down_sample\n",
    "            \n",
    "    def _get(self, images):\n",
    "        h, w = images.shape[1:3]\n",
    "        pils = [Image.fromarray(img).resize((w//self.down_sample, h//self.down_sample)) for img in images]\n",
    "        bboxes, probs = self.extractor.detect(pils)\n",
    "        facelist = [[self._rectang_crop(img, box) for box in boxes*self.down_sample] for boxes, img in zip(bboxes,images) if boxes is not None]\n",
    "        person_nums = [np.sum(prob>0.9) for prob,fss in zip(probs, bboxes) if len(fss)>0]\n",
    "        \n",
    "        assert len(person_nums) == len(facelist)\n",
    "        return facelist, person_nums\n",
    "\n",
    "\n",
    "class Dlib_extractor(Face_extractor):\n",
    "    def __init__(self, device = 'cuda:0' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.extractor = dlib.get_frontal_face_detector()\n",
    "        \n",
    "    def _get(self, images):\n",
    "        rets = [self.dlib_get_one_face(image) for image in images]\n",
    "        person_nums = [p for f,p in rets]\n",
    "        faces = [f for f,p in rets]\n",
    "        return faces, person_nums\n",
    "    \n",
    "    def dlib_get_one_face(self, image):\n",
    "        height, width = image.shape[:2]\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        faces = self.extractor(gray, 0)\n",
    "        bboxes = [[face.left(), face.top(), face.right(), face.bottom()] for face in faces]\n",
    "        facelist = [self._rectang_crop(image, box) for box in bboxes]\n",
    "        \n",
    "        return facelist, len(facelist)\n",
    "        \n",
    "    \n",
    "class Inference_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def data_transform(self):\n",
    "        # transform to Tensor\n",
    "        pre_trained_mean, pre_trained_std = [0.439, 0.328, 0.304], [0.232, 0.206, 0.201]\n",
    "        return Compose([Resize(224), ToTensor(), Normalize(pre_trained_mean, pre_trained_std)])\n",
    "    \n",
    "    \n",
    "    def TTA(self, pil_img):\n",
    "        return [pil_img, RandomHorizontalFlip(p=1)(pil_img)]\n",
    "    \n",
    "    \n",
    "    def predict(self, batch):\n",
    "        print(batch[0])\n",
    "        return 0.5\n",
    "    \n",
    "    def getx(sel, faoa):\n",
    "        l = np.float64(0)\n",
    "        r = np.float64(1)\n",
    "        while r-l>5e-8:\n",
    "            mid = (l+r)/2\n",
    "            if mid**faoa > 1-mid:\n",
    "                r = mid\n",
    "            else:\n",
    "                l=mid\n",
    "        return (r+l)/2\n",
    "\n",
    "    def give_predict(self, y):\n",
    "        y = y.clip(5e-8, 1-(5e-8))\n",
    "        faoa = np.sum(np.log(1-y))/np.sum(np.log(y))\n",
    "        ret = self.getx(faoa)\n",
    "        if ret > 0.7 and len(y[y<0.5]) > 0:\n",
    "            return self.give_predict(y[y>0.5])\n",
    "        return ret\n",
    "    \n",
    "    def test(self, shape = (1,3,224,224)):\n",
    "        return self.predict(torch.rand(shape))\n",
    "    \n",
    "\n",
    "class Model1(Inference_model):\n",
    "    def __init__(self, model_path = \"/home/kailu/best_model.pth\"):\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model = checkpoint['model']\n",
    "        self.model.eval()\n",
    "        \n",
    "    \n",
    "    def TTA(self, pil_img):\n",
    "        return [pil_img]\n",
    "    \n",
    "    \n",
    "    def predict(self, batch):\n",
    "        with torch.no_grad():\n",
    "            batch = batch.to(self.device)\n",
    "            y_pred = self.model(batch)\n",
    "            ret = self.give_predict(y_pred.cpu()[:,1].squeeze().numpy())\n",
    "        return ret\n",
    "                       \n",
    "def show(images):\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    rows = int(np.sqrt(len(images)))\n",
    "    col = int(np.ceil(len(images)/ rows))\n",
    "    fig, axes = plt.subplots(rows, col)\n",
    "    ax = np.array(axes).reshape(-1)\n",
    "    for i, img in enumerate(images):\n",
    "        if img is not None:\n",
    "            ax[i].imshow(img)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_all(video_paths, video_lables = [], video_reader=None, \n",
    "                   face_extractor=None, models=None, sample_number = 13, use_cache = False):\n",
    "    if video_reader is None and not use_cache:\n",
    "        video_reader = Video_reader()\n",
    "    if face_extractor is None and not use_cache:\n",
    "        face_extractor = MTCNN_extractor()\n",
    "    if models is None:\n",
    "        models = [Model1()]\n",
    "    if use_cache:\n",
    "        loader = Cache_loader()\n",
    "\n",
    "    def predict_one_video(file_path):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if not use_cache:\n",
    "                    frames = video_reader.extract_video(file_path)\n",
    "                    sample = np.linspace(0, len(frames) - 1, sample_number*2).round().astype(int)\n",
    "                    faces = face_extractor.get_faces(frames[sample])\n",
    "                    np.random.shuffle(faces)\n",
    "                else:\n",
    "                    faces = list(loader.extract_video(file_path).values())\n",
    "                assert len(faces) != 0\n",
    "                pils = [Image.fromarray(face)  for face in faces[:sample_number] ]\n",
    "\n",
    "                answers = []\n",
    "                for model in models:\n",
    "                    tr = model.data_transform()\n",
    "                    batch = torch.stack([tr(p) for img in pils for p in model.TTA(img)])\n",
    "                    answers.append(model.predict(batch))\n",
    "                return np.mean(answers)\n",
    "            except TypeError as e:\n",
    "                print(\"Error with \", file_path, e)\n",
    "                return 0.5\n",
    "\n",
    "    predicts = [predict_one_video(i) for i in tqdm(video_paths)]\n",
    "\n",
    "#     from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=5) as ex:\n",
    "#         predicts = list(ex.map(predict_one_video, video_paths))\n",
    "\n",
    "    if len(video_lables) == len(video_paths):\n",
    "        print(f\"loss = {log_loss(video_lables, predicts, labels=[0,1])}\")\n",
    "    return predicts\n",
    "\n",
    "def gen_data(datadf):\n",
    "    return  [base + fn for fn in datadf.index], [0 if la=='REAL' else 1 for la in datadf.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "kaggle = False\n",
    "speed_test = True\n",
    "print(f\"using {'cuda:0' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "if kaggle:\n",
    "    filenames = glob.glob('/kaggle/input/deepfake-detection-challenge/test_videos/*.mp4')\n",
    "    labels = []\n",
    "else:\n",
    "    base = '/data/deepfake/dfdc_train/'\n",
    "    metadata = pd.read_json(base + 'metadata_kailu.json').T\n",
    "    df = metadata[(metadata['split_kailu'] == 'test')]\n",
    "    filenames, labels = gen_data(df)\n",
    "      \n",
    "models = [Model1()]\n",
    "face_extractor = MTCNN_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1000 [00:35<9:43:40, 35.06s/it]\u001b[A\n",
      "  0%|          | 2/1000 [01:32<11:37:14, 41.92s/it]\u001b[A\n",
      "  0%|          | 3/1000 [02:04<10:45:17, 38.83s/it]\u001b[A\n",
      "  0%|          | 4/1000 [02:36<10:08:16, 36.64s/it]\u001b[A\n",
      "  0%|          | 5/1000 [03:04<9:25:34, 34.11s/it] \u001b[A\n",
      "  1%|          | 6/1000 [03:34<9:07:09, 33.03s/it]\u001b[A\n",
      "  1%|          | 7/1000 [05:23<15:19:39, 55.57s/it]\u001b[A\n",
      "  1%|          | 8/1000 [05:40<12:08:25, 44.06s/it]\u001b[A\n",
      "  1%|          | 9/1000 [05:58<10:01:47, 36.44s/it]\u001b[A\n",
      "  1%|          | 10/1000 [06:24<9:07:22, 33.17s/it]\u001b[A\n",
      "  1%|          | 11/1000 [06:48<8:22:16, 30.47s/it]\u001b[A\n",
      "  1%|          | 12/1000 [07:09<7:32:53, 27.50s/it]\u001b[A\n",
      "  1%|▏         | 13/1000 [07:30<7:01:42, 25.64s/it]\u001b[A\n",
      "  1%|▏         | 14/1000 [07:42<5:52:59, 21.48s/it]\u001b[A\n",
      "  2%|▏         | 15/1000 [08:03<5:53:21, 21.52s/it]\u001b[A\n",
      "  2%|▏         | 16/1000 [08:26<5:58:55, 21.89s/it]\u001b[A\n",
      "  2%|▏         | 17/1000 [08:46<5:50:27, 21.39s/it]\u001b[A\n",
      "  2%|▏         | 18/1000 [09:07<5:46:09, 21.15s/it]\u001b[A\n",
      "  2%|▏         | 19/1000 [09:12<4:27:42, 16.37s/it]\u001b[A\n",
      "  2%|▏         | 20/1000 [09:41<5:29:52, 20.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  /data1/data/deepfake/dfdc_train/igrjzpduve.mp4 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 21/1000 [10:03<5:36:28, 20.62s/it]\u001b[A\n",
      "  2%|▏         | 22/1000 [10:32<6:15:16, 23.02s/it]\u001b[A\n",
      "  2%|▏         | 23/1000 [10:55<6:18:27, 23.24s/it]\u001b[A\n",
      "  2%|▏         | 24/1000 [11:30<7:16:33, 26.84s/it]\u001b[A\n",
      "  2%|▎         | 25/1000 [11:56<7:07:56, 26.33s/it]\u001b[A\n",
      "  3%|▎         | 26/1000 [12:19<6:51:19, 25.34s/it]\u001b[A\n",
      "  3%|▎         | 27/1000 [12:41<6:35:03, 24.36s/it]\u001b[A\n",
      "  3%|▎         | 28/1000 [13:12<7:10:32, 26.58s/it]\u001b[A\n",
      "  3%|▎         | 29/1000 [13:33<6:42:24, 24.87s/it]\u001b[A\n",
      "  3%|▎         | 30/1000 [14:08<7:27:25, 27.68s/it]\u001b[A\n",
      "  3%|▎         | 31/1000 [14:29<6:56:05, 25.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  /data1/data/deepfake/dfdc_train/kykfgjhirn.mp4 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 32/1000 [14:50<6:35:00, 24.48s/it]\u001b[A\n",
      "  3%|▎         | 33/1000 [15:11<6:15:41, 23.31s/it]\u001b[A\n",
      "  3%|▎         | 34/1000 [15:32<6:02:44, 22.53s/it]\u001b[A\n",
      "  4%|▎         | 35/1000 [15:57<6:15:37, 23.35s/it]\u001b[A\n",
      "  4%|▎         | 36/1000 [16:17<5:59:13, 22.36s/it]\u001b[A\n",
      "  4%|▎         | 37/1000 [16:42<6:10:22, 23.08s/it]\u001b[A\n",
      "  4%|▍         | 38/1000 [17:03<6:01:48, 22.57s/it]\u001b[A\n",
      "  4%|▍         | 39/1000 [17:24<5:50:59, 21.91s/it]\u001b[A\n",
      "  4%|▍         | 40/1000 [17:49<6:05:47, 22.86s/it]\u001b[A\n",
      "  4%|▍         | 41/1000 [18:13<6:13:01, 23.34s/it]\u001b[A\n",
      "  4%|▍         | 42/1000 [18:40<6:28:30, 24.33s/it]\u001b[A\n",
      "  4%|▍         | 43/1000 [19:03<6:22:00, 23.95s/it]\u001b[A\n",
      "  4%|▍         | 44/1000 [19:25<6:12:16, 23.37s/it]\u001b[A\n",
      "  4%|▍         | 45/1000 [19:46<6:01:41, 22.72s/it]\u001b[A\n",
      "  5%|▍         | 46/1000 [20:08<5:56:39, 22.43s/it]\u001b[A\n",
      "  5%|▍         | 47/1000 [20:31<5:59:15, 22.62s/it]\u001b[A\n",
      "  5%|▍         | 48/1000 [20:51<5:48:56, 21.99s/it]\u001b[A\n",
      "  5%|▍         | 49/1000 [21:14<5:50:19, 22.10s/it]\u001b[A\n",
      "  5%|▌         | 50/1000 [21:38<6:01:26, 22.83s/it]\u001b[A\n",
      "  5%|▌         | 51/1000 [22:01<6:02:56, 22.95s/it]\u001b[A\n",
      "  5%|▌         | 52/1000 [22:18<5:34:14, 21.15s/it]\u001b[A\n",
      "  5%|▌         | 53/1000 [22:36<5:17:29, 20.12s/it]\u001b[A\n",
      "  5%|▌         | 54/1000 [22:57<5:21:35, 20.40s/it]\u001b[A\n",
      "  6%|▌         | 55/1000 [23:21<5:39:18, 21.54s/it]\u001b[A\n",
      "  6%|▌         | 56/1000 [23:42<5:32:37, 21.14s/it]\u001b[A\n",
      "  6%|▌         | 57/1000 [24:04<5:37:08, 21.45s/it]\u001b[A\n",
      "  6%|▌         | 58/1000 [24:28<5:50:23, 22.32s/it]\u001b[A\n",
      "  6%|▌         | 59/1000 [24:55<6:10:03, 23.60s/it]\u001b[A\n",
      "  6%|▌         | 60/1000 [25:15<5:53:14, 22.55s/it]\u001b[A\n",
      "  6%|▌         | 61/1000 [25:35<5:40:35, 21.76s/it]\u001b[A\n",
      "  6%|▌         | 62/1000 [25:53<5:22:20, 20.62s/it]\u001b[A\n",
      "  6%|▋         | 63/1000 [26:11<5:13:37, 20.08s/it]\u001b[A\n",
      "  6%|▋         | 64/1000 [26:34<5:24:37, 20.81s/it]\u001b[A\n",
      "  6%|▋         | 65/1000 [26:45<4:40:36, 18.01s/it]\u001b[A\n",
      "  7%|▋         | 66/1000 [26:56<4:07:46, 15.92s/it]\u001b[A\n",
      "  7%|▋         | 67/1000 [27:03<3:23:33, 13.09s/it]\u001b[A\n",
      "  7%|▋         | 68/1000 [27:11<2:57:59, 11.46s/it]\u001b[A\n",
      "  7%|▋         | 69/1000 [27:17<2:32:52,  9.85s/it]\u001b[A\n",
      "  7%|▋         | 70/1000 [27:25<2:26:32,  9.45s/it]\u001b[A\n",
      "  7%|▋         | 71/1000 [27:35<2:28:19,  9.58s/it]\u001b[A\n",
      "  7%|▋         | 72/1000 [27:40<2:05:26,  8.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  /data1/data/deepfake/dfdc_train/utdqvtzcwx.mp4 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 73/1000 [27:43<1:41:39,  6.58s/it]\u001b[A\n",
      "  7%|▋         | 74/1000 [27:48<1:36:53,  6.28s/it]\u001b[A\n",
      "  8%|▊         | 75/1000 [27:57<1:48:23,  7.03s/it]\u001b[A\n",
      "  8%|▊         | 76/1000 [28:05<1:53:20,  7.36s/it]\u001b[A\n",
      "  8%|▊         | 77/1000 [28:11<1:46:54,  6.95s/it]\u001b[A\n",
      "  8%|▊         | 78/1000 [28:23<2:09:45,  8.44s/it]\u001b[A\n",
      "  8%|▊         | 79/1000 [28:31<2:06:34,  8.25s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-78b974e0c642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtest_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_on_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_lables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mface_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtime_dur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"totally {time_dur} s used, {time_dur/testnum} s per video, mean = {np.mean(ret)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a579a707be3>\u001b[0m in \u001b[0;36mpredict_on_all\u001b[0;34m(video_paths, video_lables, video_reader, face_extractor, models, sample_number, use_cache)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredict_one_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     from concurrent.futures import ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a579a707be3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredict_one_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     from concurrent.futures import ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a579a707be3>\u001b[0m in \u001b[0;36mpredict_one_video\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_number\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-80f2cad8c3d3>\u001b[0m in \u001b[0;36mextract_video\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if speed_test:\n",
    "    testnum = 1000\n",
    "    start = time.time()\n",
    "    if kaggle:\n",
    "        test_filenames, test_labels = filenames[:testnum], []\n",
    "    else:\n",
    "        test_filenames, test_labels = gen_data(df.sample(testnum))\n",
    "    ret = predict_on_all(test_filenames, video_lables=test_labels, models=models, face_extractor=face_extractor)\n",
    "    time_dur = time.time()-start\n",
    "    print(f\"totally {time_dur} s used, {time_dur/testnum} s per video, mean = {np.mean(ret)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/12640 [07:34<339:27:37, 96.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with  /data1/data/deepfake/dfdc_train/aoydktojny.mp4 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/12640 [34:11<320:17:37, 91.37s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a315f2e78489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_on_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_lables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mface_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubmission_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a579a707be3>\u001b[0m in \u001b[0;36mpredict_on_all\u001b[0;34m(video_paths, video_lables, video_reader, face_extractor, models, sample_number, use_cache)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredict_one_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     from concurrent.futures import ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a579a707be3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredict_one_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     from concurrent.futures import ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a579a707be3>\u001b[0m in \u001b[0;36mpredict_one_video\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_number\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bd2ac654e740>\u001b[0m in \u001b[0;36mextract_video\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = predict_on_all(filenames, video_lables=labels, models=models, face_extractor=face_extractor)\n",
    "\n",
    "submission_df = pd.DataFrame({\"filename\": [fn.split('/')[-1] for fn in filenames], \"label\": predictions})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
