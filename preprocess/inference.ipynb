{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Kernel Demo\n",
    "\n",
    "This is the kernel I’ve used for my recent submissions. It takes about 5-6 hours on the test set, using only CPU. \n",
    "\n",
    "I’ve provided this kernel because a lot of people have problems making submissions. This method works and has never errored out for me. (Although I haven't tried making a submission using the GPU yet -- so no guarantees there.)\n",
    "\n",
    "It uses BlazeFace for face extraction (see also [my BlazeFace kernel](https://www.kaggle.com/humananalog/starter-blazeface-pytorch)) and ResNeXt50 as the classifier model.\n",
    "\n",
    "We take the average prediction over 17 frames from each video. (Why 17? Using more frames makes the kernel slower, but doesn't appear to improve the score much. I used an odd number so we don't always land on even frames.)\n",
    "\n",
    "**Please use this kernel only to learn from...** Included is the checkpoint for a ResNeXt50 model that hasn't really been trained very well yet. I'm sure you can improve on it by training your own model!\n",
    "\n",
    "You could use the included trained weights to get yourself an easy top-50 score on the leaderboard (as of 24 Jan 2020) but it’s nicer to use it as a starting point for your own work. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Local = True\n",
    "\n",
    "if Local:\n",
    "    data_path = \"/data1/data/deepfake/dfdc_train/\"\n",
    "    metadata: pd.DataFrame = pd.read_json(data_path + \"metadata_kailu.json\").T\n",
    "    test_videos = sorted([data_path + x for x in metadata.index])\n",
    "else:\n",
    "    test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
    "    test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.3.0\n",
      "CUDA version: 10.0.130\n",
      "cuDNN version: 7603\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
    "sys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Local:\n",
    "    from blazeface import BlazeFace\n",
    "    facedet = BlazeFace().to(gpu)\n",
    "    facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
    "    facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
    "    _ = facedet.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Local:\n",
    "    from helpers.read_video_1 import VideoReader\n",
    "    from helpers.face_extract_1 import FaceExtractor\n",
    "    \n",
    "    frames_per_video = 43\n",
    "    \n",
    "    video_reader = VideoReader()\n",
    "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
    "    face_extractor = FaceExtractor(video_read_fn, facedet)\n",
    "else:\n",
    "    from model1.utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize, RandomHorizontalFlip, ToTensor, ToPILImage, Compose\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)\n",
    "flip = Compose([ToPILImage(), RandomHorizontalFlip(p=1), ToTensor(), normalize_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
    "    h, w = img.shape[:2]\n",
    "    if w > h:\n",
    "        h = h * size // w\n",
    "        w = size\n",
    "    else:\n",
    "        w = w * size // h\n",
    "        h = size\n",
    "\n",
    "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def make_square_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    t = 0\n",
    "    b = size - h\n",
    "    l = 0\n",
    "    r = size - w\n",
    "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyResNeXt(models.resnet.ResNet):\n",
    "    def __init__(self, training=True):\n",
    "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
    "                                        layers=[3, 4, 6, 3], \n",
    "                                        groups=32, \n",
    "                                        width_per_group=4)\n",
    "        self.fc = nn.Linear(2048, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/kaggle/input/deepfakes-inference-demo/resnext.pth\", map_location=gpu)\n",
    "\n",
    "model = MyResNeXt().to(gpu)\n",
    "model.load_state_dict(checkpoint)\n",
    "_ = model.eval()\n",
    "\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    # We keep the aspect ratio intact and add zero\n",
    "                    # padding if necessary.                    \n",
    "                    resized_face = isotropically_resize_image(face, input_size)\n",
    "                    resized_face = make_square_image(resized_face)\n",
    "\n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "\n",
    "    #                 Test time augmentation: horizontal flips.\n",
    "    #                 TODO: not sure yet if this helps or not\n",
    "                    x[n] = cv2.flip(resized_face, 1)\n",
    "                    n += 1\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=gpu).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction, then take the average.\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(x)\n",
    "                    y_pred = torch.sigmoid(y_pred.squeeze()).cpu()\n",
    "\n",
    "                    return give_predict(y_pred[:n])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "\n",
    "    return 0.5\n",
    "\n",
    "def give_predict(y):\n",
    "    y = y.numpy().clip(5e-8, 1-(5e-8))\n",
    "    return np.mean(y)\n",
    "\n",
    "#     y = y.numpy().clip(5e-8, 1-(5e-8))\n",
    "#     num = len(y)\n",
    "#     return np.exp(np.log(y).sum()/num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def predict_on_video_set(videos, num_workers):\n",
    "    def process_file(i):\n",
    "        filename = videos[i]\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video*2)\n",
    "        return y_pred\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        predictions = ex.map(process_file, range(len(videos)))\n",
    "\n",
    "    return list(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test\n",
    "\n",
    "The leaderboard submission must finish within 9 hours. With 4000 test videos, that is `9*60*60/4000 = 8.1` seconds per video. So if the average time per video is greater than ~8 seconds, the kernel will be too slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_test = True  # you have to enable this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 20.701058 sec. Average per video: 4.140212 sec.\n"
     ]
    }
   ],
   "source": [
    "if speed_test:\n",
    "    start_time = time.time()\n",
    "    speedtest_videos = test_videos[-5:]\n",
    "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_on_video_set(test_videos, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"filename\": test_videos[:], \"label\": predictions})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.577892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.003680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.685165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.065221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.933871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename     label\n",
       "0  aassnaulhq.mp4  0.577892\n",
       "1  aayfryxljh.mp4  0.003680\n",
       "2  acazlolrpz.mp4  0.685165\n",
       "3  adohdulfwb.mp4  0.065221\n",
       "4  ahjnxtiamx.mp4  0.933871"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}